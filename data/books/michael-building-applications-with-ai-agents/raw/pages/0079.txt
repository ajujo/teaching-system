reviews, edits if necessary, and approves the reply before sending. This acceler‐
ates response time while maintaining human judgment.
Agent
The agent autonomously handles routine queries—such as password resets, order
tracking, or FAQs—without human intervention, escalating only complex or sen‐
sitive issues to human agents. Users are notified of agent actions but do not need
to approve each message for standard interactions.
These three modes coexist within the same customer support system, empowering
teams to adjust autonomy based on query complexity, customer profile, and organi‐
zational trust in AI. This same autonomy slider pattern can extend to any field where
workflows benefit from fluidly shifting between manual execution, AI assistance, and
full agentic automation. This spectrum of autonomy must be consciously designed
into agent experiences. Without it, agents risk feeling either underpowered (if they
require too much manual input) or overbearing (if they act without user consent in
sensitive contexts). To integrate an autonomy slider effectively, consider the following
design principles:
Expose degrees of autonomy clearly
Users should understand the available levels of agent independence, from manual
to assisted to autonomous. Label these modes in intuitive language, such as
“Manual,” “Assist,” and “Auto,” and explain their implications.
Enable seamless transitions
Users must be able to shift between autonomy levels effortlessly as their confi‐
dence, context, or workload changes. For instance, a toggle or slider in the inter‐
face should offer a quick transition from review mode to auto-approve mode.
Provide predictable and transparent behavior at each level
Each autonomy level should have well-defined behaviors. In partial automation,
for example, the agent may draft an output but require explicit user approval
before execution. In full autonomy, it should still provide status updates and
options to intervene.
Communicate the risks and benefits of each level
Users should be aware of what they gain or risk by increasing agent autonomy.
For critical tasks, it may be advisable to require an explicit user confirmation
before enabling full autonomy.
Adapt autonomy based on user trust and competence
Intelligent systems can gradually suggest higher autonomy levels as users gain
trust and as the agent demonstrates reliability. For example, after 10 successful
uses in manual mode, the system might suggest trying assist mode to save time.
Interaction Modalities 
| 
57
