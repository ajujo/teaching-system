DPO_DATA      = "training_data/dpo_it_help_desk_training_data.jsonl"
OUTPUT_DIR    = "phi3-mini-helpdesk-dpo"
# 1⃣ Model + tokenizer
tok = AutoTokenizer.from_pretrained(BASE_SFT_CKPT, padding_side="right",
                                    trust_remote_code=True)
logger = logging.getLogger(__name__)
if not os.path.exists(BASE_SFT_CKPT):
    logger.warning(f'''Local path not found; will 
        attempt to download {BASE_SFT_CKPT} from the Hub.''')
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,               
    bnb_4bit_use_double_quant=True,  
    bnb_4bit_compute_dtype=torch.bfloat16
)
base = AutoModelForCausalLM.from_pretrained(
    BASE_SFT_CKPT,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=bnb_config
)
lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
   target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj",
                    "up_proj", "down_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(base, lora_cfg)
print("✅  Phi-3 loaded:", model.config.hidden_size, "hidden dim")
Next, we load our dataset containing ranked pairs. Each example includes a prompt, a
preferred (“chosen”) response, and a less preferred (“rejected”) response. This struc‐
ture enables the model to learn which outputs to favor during training:
# Load DPO dataset with ranked pairs
#    Each row should include: {"prompt": ..., "chosen": ..., "rejected": ...}
dataset = load_dataset("json", 
    data_files="training_data/dpo_it_help_desk_training_data.jsonl", 
    split="train")
With our data prepared, we define training hyperparameters and configure DPO. The
beta parameter adjusts how strongly the model prioritizes the preferred response
during optimization:
Parametric Learning: Fine-Tuning 
| 
159
