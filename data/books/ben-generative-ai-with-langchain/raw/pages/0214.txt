Chapter 5
187
print(result.tool_calls)
>> [{'name': 'google_search', 'args': {'query': 'age of Donald Trump'}, 
'id': '6ab0de4b-f350-4743-a4c1-d6f6fcce9d34', 'type': 'tool_call'}]
Keep in mind that some model providers might return non-empty content even in the case of 
tool calling (for example, there might be reasoning traces on why the model decided to call a 
tool). You need to look at the model provider specification to understand how to treat such cases.
As we can see, an LLM returned an array of tool-calling dictionaries—each of them contains a 
unique identifier, the name of the tool to be called, and a dictionary with arguments to be provided 
to this tool. Let’s move to the next step and invoke the model again:
from langchain_core.messages import SystemMessage, HumanMessage, 
ToolMessage
tool_result = ToolMessage(content="Donald Trump ' Age 78 years June 14, 
1946\n", tool_call_id=step1.tool_calls[0]["id"])
step2 = llm.invoke([
   HumanMessage(content=question), step1, tool_result], tools=[search_
tool])
assert len(step2.tool_calls) == 0
print(step2.content)
>> Donald Trump is 78 years old.
ToolMessage is a special message on LangChain that allows you to feed the output of a tool exe­
cution back to the model. The content field of such a message contains the tool’s output, and a 
special field tool_call_id maps it to the specific tool calling that was generated by the model. 
Now, we can send the whole sequence (consisting of the initial output, the step with tool calling, 
and the output) back to the model as a list of messages.
It might be odd to always pass a list of tools to the LLM (since, typically, such a list is fixed for 
a given workflow). For that reason, LangChain Runnables offer a bind method that memorizes 
arguments and adds them to every further invocation. Take a look at the following code:
llm_with_tools = llm.bind(tools=[search_tool])
llm_with_tools.invoke(question)
