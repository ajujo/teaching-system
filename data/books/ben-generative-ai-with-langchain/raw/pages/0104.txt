Chapter 3
77
job_description: str = ...  # put your JD here
prompt_template = (
   "Given a job description, decide whether it suits a junior Java 
developer."
   "\nJOB DESCRIPTION:\n{job_description}\n"
)
result = llm.invoke(prompt_template.format(job_description=job_
description))
print(result.content)
>> No, this job description is not suitable for a junior Java 
developer.\n\nThe key reasons are:\n\n* … (output reduced)
As you can see, the output of the LLM is free text, which might be difficult to parse or interpret in 
subsequent pipeline steps. What if we add a specific instruction to a prompt?
prompt_template_enum = (
   "Given a job description, decide whether it suits a junior Java 
developer."
   "\nJOB DESCRIPTION:\n{job_description}\n\nAnswer only YES or NO."
)
result = llm.invoke(prompt_template_enum.format(job_description=job_
description))
print(result.content)
>> NO
Now, how can we parse this output? Of course, our next step can be to just look at the text and 
have a condition based on a string comparison. But that won’t work for more complex use cases – 
for example, if the next step expects the output to be a JSON object. To deal with that, LangChain 
offers plenty of OutputParsers that take the output generated by the LLM and try to parse it into a 
desired format (by checking a schema if needed) – a list, CSV, enum, pandas DatafFrame, Pydantic 
model, JSON, XML, and so on. Each parser implements a BaseGenerationOutputParser interface, 
which extends the Runnable interface with an additional parse_result method.
Let’s build a parser that parses an output into an enum:
from enum import Enum
from langchain.output_parsers import EnumOutputParser
from langchain_core.messages import HumanMessage
