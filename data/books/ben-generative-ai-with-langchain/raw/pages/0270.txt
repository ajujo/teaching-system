Chapter 6
243
If we switch to the update mode, we’ll get a dictionary where the key is the node’s name (remem­
ber that parallel nodes can be called within a single super-step) and a corresponding update to 
the state sent by this node:
async for _, event in research_agent.astream({"question": question, 
"options": options}, stream_mode=["updates"]):
 node = list(event.keys())[0]
 print(node, len(event[node].get("messages", [])))
>> agent 1
tools 2
agent 1
LangGraph stream always emits a tuple where the first value is a stream mode (since you can 
pass multiple modes by adding them to the list).
Then you need an astream_events method that streams back events happening within the 
nodes – not just tokens generated by the LLM but any event available for a callback:
seen_events = set([])
async for event in research_agent.astream_events({"question": question, 
"options": options}, version="v1"):
 if event["event"] not in seen_events:
   seen_events.add(event["event"])
print(seen_events)
>> {'on_chat_model_end', 'on_chat_model_stream', 'on_chain_end', 'on_
prompt_end', 'on_tool_start', 'on_chain_stream', 'on_chain_start', 'on_
prompt_start', 'on_chat_model_start', 'on_tool_end'}
You can find a full list of the events at https://python.langchain.com/docs/concepts/
callbacks/#callback-events.
Handoffs
So far, we have learned that a node in LangGraph does a chunk of work and sends updates to a 
common state, and an edge controls the flow – it decides which node to invoke next (in a deter­
ministic manner or based on the current state). When implementing multi-agent architectures, 
your nodes can be not only functions but other agents, or subgraphs (with their own state). You 
might need to combine state updates and flow controls.
