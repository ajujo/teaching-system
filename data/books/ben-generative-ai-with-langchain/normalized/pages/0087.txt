First Steps with LangChain
60
The background is a grid of various digital interfaces, displaying graphs,
charts, and other abstract data visualizations. These elements are all in
shades of blue, creating a cool, technological ambiance that complements
the robot's appearance. The displays vary in size and complexity, adding
to the sense of a sophisticated control panel or monitoring system. The
combination of the robot and the background suggests a theme of advanced
robotics, artificial intelligence, or data analysis.
As multimodal inputs typically have a large size, sending raw bytes as part of your request might
not be the best idea. You can send it by reference by pointing to the blob storage, but the specific
type of storage depends on the model's provider. For example, Gemini accepts multimedia input
as a reference to Google Cloud Storage - a blob storage service provided by Google Cloud.
prompt = [
   {"type": "text", "text": "Describe the video in a few sentences."},
   {"type": "media", "file_uri": video_uri, "mime_type": "video/mp4"},
]
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)
Exact details on how to construct a multimodal input might depend on the provider of the LLM
(and a corresponding LangChain integration handles a dictionary corresponding to a part of a
content field accordingly). For example, Gemini accepts an additional "video_metadata" key
that can point to the start and/or end offset of a video piece to be analyzed:
offset_hint = {
           "start_offset": {"seconds": 10},
           "end_offset": {"seconds": 20},
       }
prompt = [
   {"type": "text", "text": "Describe the video in a few sentences."},
   {"type": "media", "file_uri": video_uri, "mime_type": "video/mp4",
"video_metadata": offset_hint},
]
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)
