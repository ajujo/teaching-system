Production-Ready LLM Deployment and Observability
360
    chunks = text_splitter.split_documents(docs)
    print(f"Generated {len(chunks)} chunks")
    return chunks
These Ray remote functions enable distributed processing:
•
preprocess_documents splits documents into manageable chunks.
•
embed_chunks converts text chunks into vector embeddings and builds FAISS indices.
•
The @ray.remote decorator makes these functions run in separate Ray workers.
Our main index-building function looks like this:
def embed_chunks_with_progress(More actions
    chunks: List[Document],
    batch_id: int,
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
) -> FAISS:
    """Embed a batch of document chunks and create a FAISS index.

    Args:
        chunks: List of document chunks to embed.
        batch_id: Identifier for this batch (for progress tracking).
        model_name: Name of the embedding model to use.

    Returns:
        FAISS index containing the embedded chunks.
    """
    print(f"[Batch {batch_id}] Starting embedding of {len(chunks)}
chunks")
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    result = FAISS.from_documents(chunks, embeddings)
    print(f"[Batch {batch_id}] Completed embedding")
    return result
def build_index(
    base_url: str = "https://python.langchain.com/docs/tutorials/",
    batch_size: int = 10,
    max_depth: int = 2,
    embedding_batch_size: int = 500,
