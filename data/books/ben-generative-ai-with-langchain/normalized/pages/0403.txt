Production-Ready LLM Deployment and Observability
376
Launch the local development server:
langgraph dev
This starts a server at http://localhost:2024 with:
•
API endpoint
•
API documentation
•
A link to the LangGraph Studio web UI for debugging
Test your application using the SDK:
from langgraph_sdk import get_client
client = get_client(url="http://localhost:2024")
# Stream a response from the agent
async for chunk in client.runs.stream(
    None,  # Threadless run
    "agent",  # Name of assistant defined in langgraph.json
    input={
        "messages": [{
            "role": "human",
            "content": "What is LangGraph?",
        }],
    },
    stream_mode="updates",
):
    print(f"Receiving event: {chunk.event}...")
    print(chunk.data)
The local development server uses an in-memory store for state, making it suitable for rapid
development and testing. For a more production-like environment with persistence, you can use
langgraph up instead of langgraph dev.
To deploy a LangGraph application to production, you need to configure your application properly.
Set up the langgraph.json configuration file:
