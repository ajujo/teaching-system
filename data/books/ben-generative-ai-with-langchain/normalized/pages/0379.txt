Production-Ready LLM Deployment and Observability
352
•
Continuous adversarial red-teaming: Maintain a library of context-specific attack
prompts and regularly test your deployment to catch regressions and new injection pat­
terns.
•
Align stakeholders on security benchmarks: Adopt or reference OWASP's LLM Security
Verification Standard to keep developers, security, and management aligned on evolving
best practices.
LLMs can unintentionally expose sensitive information that users feed into them. Samsung Elec­
tronics famously banned employee use of ChatGPT after engineers pasted proprietary source code
that later surfaced in other users' sessions (Forbes. Samsung Bans ChatGPT Among Employees After
Sensitive Code Leak. 2023).
Beyond egress risks, data‐poisoning attacks embed "backdoors" into models with astonishing
efficiency. Researchers Nicholas Carlini and Andreas Terzis, in their 2021 paper Poisoning and
Backdooring Contrastive Learning, have shown that corrupting as little as 0.01% of a training data­
set can implant triggers that force misclassification on demand. To guard against these stealthy
threats, teams must audit training data rigorously, enforce provenance controls, and monitor
models for anomalous behavior.
We can now explore the practical aspects of deploying LLM applications to production environ­
ments. The next section will cover the various deployment options available and their relative
advantages.
Generally, to mitigate security threats in production, we recommend treating the
LLM as an untrusted component: separate system prompts from user text in distinct
context partitions; filter inputs and validate outputs against strict schemas (for
instance, enforcing JSON formats); and restrict the model's authority to only the
tools and APIs it truly needs.
In RAG systems, additional safeguards include sanitizing documents before embed­
ding, applying least-privilege access to knowledge partitions, and imposing rate
limits or token budgets to prevent denial-of-service attacks. Finally, security teams
should augment standard testing with adversarial red-teaming of prompts, mem­
bership inference assessments for data leakage, and stress tests that push models
toward resource exhaustion.
