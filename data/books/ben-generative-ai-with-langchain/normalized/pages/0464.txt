Appendix
437
Now, back to our code. We still need to run two separate steps if we want to pass video_uri as an
input argument. Of course, we can wrap these steps as a Python function, but as an alternative,
we merge everything into a single chain:
from langchain_core.runnables import RunnableLambda
create_inputs_chain = RunnableLambda(lambda x: _create_input_
messages(**x))
map_step_chain = create_inputs_chain | RunnableLambda(lambda x: map_chain.
batch(x, config={"max_concurrency": 3}))
summaries = map_step_chain.invoke({"video_uri": video_uri})
Now let's merge all summaries provided into a single prompt and ask an LLM to prepare a final
summary:
def _merge_summaries(summaries: list[str], interval_secs: int = 600,
**kwargs) -> str:
    sub_summaries = []
    for i, summary in enumerate(summaries):
        sub_summary = (
            f"Summary from sec {i*interval_secs} to sec {(i+1)*interval_
secs}:"
            f"\n{summary}\n"
        )
        sub_summaries.append(sub_summary)
    return "".join(sub_summaries)
reduce_prompt = PromptTemplate.from_template(
    "You are given a list of summaries that"
    "of a video splitted into sequential pieces.\n"
    "SUMMARIES:\n{summaries}"
    "Based on that, prepare a summary of a whole video."
)
reduce_chain = RunnableLambda(lambda x: _merge_summaries(**x)) | reduce_
prompt | llm | StrOutputParser()
final_summary = reduce_chain.invoke({"summaries": summaries})
