Chapter 3
93
generations = []
for _ in range(20):
 generations.append(final_chain.invoke({"question": "Solve equation
2*x**2-96*x+1152"}, temperature=2.0).strip())
from collections import Counter
print(Counter(generations).most_common(1)[0][0])
>> x = 24
As you can see, we first created a list containing multiple outputs generated by an LLM for the
same input and then created a Counter class that allowed us to easily find the most common
element in this list, and we took it as a final answer.
Now that we have learned how to efficiently organize your prompt and use different prompt
engineering approaches with LangChain, let's talk about what can we do if prompts become too
long and they don't fit into the model's context window.
Working with short context windows
A context window of 1 or 2 million tokens seems to be enough for almost any task we could imagine.
With multimodal models, you can just ask the model questions about one, two, or many PDFs,
images, or even videos. To process multiple documents (for summarization or question answering),
you can use what's known as the stuff approach. This approach is straightforward: use prompt
templates to combine all inputs into a single prompt. Then, send this consolidated prompt to an
LLM. This works well when the combined content fits within your model's context window. In the
coming chapter, we'll discuss further ways of using external data to improve models' responses.
Switching between model providers
Different providers might have slightly different guidance on how to construct the
best working prompts. Always check the documentation on the provider's side -
for example, Anthropic emphasizes the importance of XML tags to structure your
prompts. Reasoning models have different prompting guidelines (for example, typÂ­
ically, you should not use either CoT or few-shot prompting with such models).
Last but not least, if you're changing the model provider, we highly recommend
running an evaluation and estimating the quality of your end-to-end application.
