First Steps with LangChain
44
with_transformation = prompt | llm | (lambda x: x.upper()) |
StrOutputParser()
For more complex workflows, you can incorporate branching logic:
decision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {
    "summarize": summarize_chain,
    "analyze": analyze_chain
}
Non-Runnable elements like functions and dictionaries are automatically converted to appro­
priate Runnable types:
# Function to Runnable
length_func = lambda x: len(x)
chain = prompt | length_func | output_parser
# Is converted to:
chain = prompt | RunnableLambda(length_func) | output_parser
The flexible, composable nature of LCEL will allow us to tackle real-world LLM application chal­
lenges with elegant, maintainable code.
Simple workflows with LCEL
As we've seen, LCEL provides a declarative syntax for composing LLM application components
using the pipe operator. This approach dramatically simplifies workflow construction compared
to traditional imperative code. Let's build a simple joke generator to see LCEL in action:
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
# Create components
prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
llm = ChatOpenAI()
output_parser = StrOutputParser()
# Chain them together using LCEL
chain = prompt | llm | output_parser
#  Execute the workflow with a single call
