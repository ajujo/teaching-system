{
  "$schema": "attempt_v1",
  "attempt_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-a01",
  "exercise_set_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07",
  "unit_id": "paul-llm-engineer-s-handbook-ch01-u01",
  "book_id": "paul-llm-engineer-s-handbook",
  "created_at": "2026-02-06T11:38:31.893673+00:00",
  "status": "graded",
  "answers": [
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q01",
      "response": 1
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q02",
      "response": true
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q03",
      "response": "Porque un MVP te permite validar rápido que el LLM Twin aporta valor con el mínimo de trabajo y riesgo, antes de invertir en una versión “perfecta”."
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q04",
      "response": 1
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q05",
      "response": true
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q06",
      "response": "El **pipeline de inferencia** es el componente que **pone el LLM Twin a funcionar en tiempo de uso**: toma la consulta del usuario, **prepara el contexto necesario** (por ejemplo, recuperando información de una base de datos/vector DB si hay RAG), construye el prompt/entrada final y **genera la respuesta** del modelo, aplicando reglas como formato, seguridad (guardrails), memoria y control de calidad/latencia."
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q07",
      "response": 2
    },
    {
      "exercise_id": "paul-llm-engineer-s-handbook-ch01-u01-ex07-q08",
      "response": false
    }
  ],
  "total_questions": 8
}