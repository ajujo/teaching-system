Chapter 10
383
That’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest part is finding 
the correct configuration to fit your needs while reducing your infrastructure’s costs. Depending 
on AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly 
from your .env file and deploy the model with a different configuration without touching the 
code. For example, our default values use a single GPU instance of type ml.g5.xlargeGPU. If you 
want more replicas, you can tweak the GPUS and SM_NUM_GPUS settings or change your instance 
type by changing the GPU_INSTANCE_TYPE variable.
After deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker 
dashboard in AWS to visualize it. First, in the left panel, click on SageMaker dashboard, and then 
in the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.
Figure 10.6: AWS SageMaker Inference endpoints example
Before deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-
erated a user role by running poetry poe create-sagemaker-role and an exe-
cution role by running poetry poe create-sagemaker-execution-role. Also, 
ensure you update your AWS_* environment variables in your .env file with the 
credentials generated by the two steps. You can find more details on this aspect in 
the repository’s README file.
