Chapter 5
193
An important aspect of synthetic data generation is the ability to control various attributes of the 
generated data. This includes factors such as the complexity of the instructions, the length of the 
responses, the tone or style of the language used, and the specific topics or domains covered. By 
fine-tuning these parameters, it’s possible to create datasets that are tailored to specific training 
objectives or that complement existing datasets in targeted ways. Structured generation using 
libraries like Outlines can also be beneficial to adhere to specific formats.
Furthermore, synthetic data generation can be particularly useful for addressing biases and gaps 
in existing datasets. By carefully designing the generation process, it’s possible to create more 
balanced and inclusive datasets that represent a wider range of perspectives, topics, and language 
styles. This can help in training LLMs that are more equitable and capable of serving diverse user 
bases.
However, synthetic data generation also comes with challenges. One primary concern is the 
potential for the generated data to inherit biases or errors from the underlying language model 
used for generation. To mitigate this, many approaches incorporate human oversight, diverse 
prompts, and additional filtering mechanisms to ensure the quality and appropriateness of the 
generated data.
Another consideration is the need for the generated data to be sufficiently diverse and challeng-
ing. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity 
required to train a robust LLM. Advanced techniques in synthetic data generation often focus on 
creating varied and nuanced instruction-response pairs that can push the boundaries of what 
the model can learn.
Data augmentation
In this context, data augmentation refers to the process of increasing both the quantity and 
the quality of data samples. Unlike data generation, we use pre-existing instruction samples 
as inputs in this stage. While it is possible to upsample pairs of instructions and answers, data 
augmentation is mostly used to increase the quality of existing samples. In particular, it focuses 
on two aspects: diversity and complexity.
A pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple 
instructions into more qualitative ones. The evolved instructions can then be used to generate 
answers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth 
evolving.
