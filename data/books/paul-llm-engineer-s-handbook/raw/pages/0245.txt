Supervised Fine-Tuning
214
Mathematically, this can be represented as:
ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğµğµğµğµ
Here, ğ‘Šğ‘Š is the original weight matrix, ğµğµ and ğ´ğ´ are the LoRA matrices, and ğ‘Šğ‘Šğ‘Š is the effective weight 
matrix used during inference.
The dimensions of matrices A and B are chosen such that their product has the same shape as 
ğ‘Šğ‘Š, but with a much lower rank. This rank, typically denoted as ğ‘Ÿğ‘Ÿ , is a crucial hyperparameter 
in LoRA. During training, the original weights ğ‘Šğ‘Š remain frozen, while only ğ´ğ´ and ğµğµ are updated. 
This approach significantly reduces the number of trainable parameters, leading to substantial 
memory savings and faster training times.
To implement LoRA effectively, we need to select the correct hyperparameters and target modules. 
LoRA comes with two hyperparameters:
â€¢	
Rank (ğ‘Ÿğ‘Ÿ ): Determines the size of the LoRA matrices. A common starting point is ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ, but 
values up to 256 have shown good results in some cases. Larger ranks may capture more 
diverse tasks but could lead to overfitting.
â€¢	
Alpha (ğ›¼ğ›¼): A scaling factor applied to the LoRA update. In practice, we update the frozen 
weights ğ‘Šğ‘Š by a factor of ğ›¼ğ›¼ğ›¼ğ›¼ğ›¼ . This is why a common heuristic is to set ğ›¼ğ›¼ to twice the 
value of ğ‘Ÿğ‘Ÿ , effectively applying a scaling factor of 2 to the LoRA update. You can experiment 
with different ratios in case of overfitting or underfitting.
In addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually 
set between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.
LoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily fo-
cused on modifying the attention mechanism, specifically the query (Q) and value (V) matrices 
in transformer layers. However, experiments have demonstrated significant benefits in extending 
LoRAâ€™s application to other key components of the model. These additional target modules include:
â€¢	
Key (K) matrices in attention layers
â€¢	
Output projection layers (often denoted as O) in attention mechanisms
â€¢	
Feed-forward or Multi-Layer Perceptron (MLP) blocks between attention layers
â€¢	
Linear output layers
However, itâ€™s important to note that increasing the number of LoRA-adapted modules also in-
creases the number of trainable parameters and, consequently, the memory requirements.
