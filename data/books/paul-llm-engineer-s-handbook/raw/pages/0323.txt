Inference Optimization
292
You can see an illustration of this technique in Figure 8.2:
Figure 8.2 â€“ Illustration of the KV cache
When a new token is generated, only the key and value for that single token need to be computed 
and added to the cache. The KV cache is an immediate optimization that is implemented in every 
popular tool and library. Some implementations maintain a separate KV cache for each layer of 
the model.
The size of the KV cache scales with the number of tokens (ğ‘›ğ‘›ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡) and several model dimensions, 
like the number of layers (ğ‘›ğ‘›ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™), the number of attention heads (ğ‘›ğ‘›â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ), their dimension (dimâ„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’
), and the precision of the parameters in bytes (ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘):
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ¾ğ¾ğ¾ğ¾ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) = 2ğ‘›ğ‘›ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğ‘›ğ‘›ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘›ğ‘›â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ dimâ„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘›ğ‘›ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
For a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence 
lengths (higher than 2,048 tokens). Larger models with more layers and higher embedding di-
mensions will see even greater memory requirements.
