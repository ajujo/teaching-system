Inference Pipeline Deployment
368
Let’s review the interface of the inference pipeline, which is defined by the feature/training/
inference (FTI) architecture. For the pipeline to run, it needs two things:
•	
Real-time features used for RAG, generated by the feature pipeline, which is queried from 
our online feature store, more concretely from the Qdrant vector database (DB)
•	
A fine-tuned LLM generated by the training pipeline, which is pulled from our model 
registry
With that in mind, the flow of the ML service looks as follows, as illustrated in Figure 10.4:
1.	
A user sends a query through an HTTP request.
2.	
The user’s input retrieves the proper context by leveraging the advanced RAG retrieval 
module implemented in Chapter 4.
3.	
The user’s input and retrieved context are packed into the final prompt using a dedicated 
prompt template.
4.	
The prompt is sent to the LLM microservice through an HTTP request.
5.	
The business microservices wait for the generated answer.
6.	 After the answer is generated, it is sent to the prompt monitoring pipeline along with the 
user’s input and other vital information to monitor.
7.	
Ultimately, the generated answer is sent back to the user.
Now, let’s explore what tech stack we used to implement the architecture presented in Figure 
10.4. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model 
registry. By doing so, we can publicly share our model with everyone who is testing the code from 
this book. Thus, you can easily use the model we provided if you don’t want to run the training 
pipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some 
of the most beautiful aspects of storing your model in a model registry.
We will implement the business microservice in FastAPI because it’s popular, easy to use, and fast. 
The LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s 
integration with Hugging Face’s Deep Learning Containers (DLCs) to deploy the model. We will 
discuss Hugging Face’s DLCs in the next section, but intuitively, it is an inference engine used to 
optimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, 
but we will look over that module only in Chapter 11.
