Chapter 11
455
   "temperature": settings.TEMPERATURE_INFERENCE,
   "prompt_tokens": compute_num_tokens(prompt),
   "total_tokens": compute_num_tokens(answer),
  
}
 	
	
)
    return answer
There are three main aspects that we should constantly monitor:
•	
Model configuration: Here, we should consider both the LLM and other models used 
within the RAG layer. The most critical aspects of logging are the model IDs, but you can 
also capture other important information that significantly impacts the generation, such 
as the temperature.
•	
Total number of tokens: It’s critical to constantly analyze the statistics of the number of 
tokens generated by your input prompts and total tokens, as this significantly impacts 
your serving costs. For example, if the average of the total number of tokens generated 
suddenly increases, it’s a strong signal that you have a bug in your system that you should 
investigate.
•	
The duration of each step: Tracking the duration of each step within your trace is essential 
to finding bottlenecks within your system. If the latency of a specific request is abnormally 
large, you quickly have access to a report that helps you find the source of the problem.
Alerting
Using ZenML, you can quickly implement an alerting system on any platform of your liking, such 
as email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger 
a notification when the pipeline fails or the training has finished successfully:
from zenml import get_pipeline_context, pipeline
@pipeline(on_failure=notify_on_failure)
def training_pipeline(…):
…
notify_on_success()
