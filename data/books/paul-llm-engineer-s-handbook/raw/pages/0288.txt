Chapter 6
257
•	
Accuracies: This metric represents the percentage of times the model correctly identifies 
the chosen answers. We want this accuracy to gradually increase during training, but it 
doesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, in-
dicates that the preference dataset might be too easy for the model. While the LLM can 
still learn from such a dataset, it might be beneficial to add more challenging examples.
In general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex pro-
cess, involving a reference model. However, it’s also significantly easier to use than PPO and other 
RLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned 
model, you can experiment with different ranks, beta parameters, learning rates, and number of 
epochs to see which experiment best captures your preferences.
While this is not the purpose of this chapter, it is possible to automate the evaluation of models 
designed to imitate a writing style. A possible solution consists of comparing the distribution of 
words in the text generated by different models (SFT and DPO) with our ground-truth dataset. 
In this example, we expect the SFT model to output a lot of words that are overrepresented in 
GPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer 
to the chosen answers.
Summary
This chapter explored preference alignment techniques for improving LLMs. It introduced the 
concept of preference datasets, explaining their structure and importance in capturing nuanced 
human preferences. We implemented our own custom preference data generation pipeline by 
comparing original and AI-generated text from real articles. This pipeline can be reused and 
customized based on your use case.
We also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a 
simpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to 
fine-tune our TwinLlama-3.1-8B model from Chapter 5. Our step-by-step tutorial gave practical 
instructions for training the model, as well as highlighting key differences from SFT. The final 
model is available on the Hugging Face Hub.
In the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges 
and current approaches in assessing LLM performance. We’ll cover the creation of domain-specific 
evaluation sets, examine why evaluation remains a persistent problem in the field, and introduce 
the concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will 
conclude with a comprehensive evaluation pipeline, providing a structured framework for con-
sistent and effective LLM evaluation.
