Inference Pipeline Deployment
384
After clicking the Endpoints button, you will see your twin endpoint in a Creating or Created 
status, as seen in Figure 10.7. After clicking on it, you can look at the endpoint’s logs in CloudWatch 
and monitor the CPU, memory, disk, and GPU utilization. Also, they provide an excellent way to 
monitor all the HTTP errors, such as 4XX and 5XX, in one place.
Figure 10.7: AWS SageMaker twin inference endpoint example
Calling the AWS SageMaker Inference endpoint
Now that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service. 
To do so, we will write two classes that will help us prepare the prompt for SageMaker, call the 
inference endpoint through HTTP requests, and decode the results in a way the client can work 
with. All the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/
inference. It all starts with the following example:
text = "Write me a post about AWS SageMaker inference endpoints."
llm = LLMInferenceSagemakerEndpoint(
        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE
    )
Answer = InferenceExecutor(llm, text).execute()
Quick tip: Need to see a high-resolution version of this image? Open this book 
in the next-gen Packt Reader or view it in the PDF/ePub copy. 
 The next-gen Packt Reader and a free PDF/ePub copy of this book are included 
with your purchase. Unlock them by scanning the QR code below or visiting https://
www.packtpub.com/unlock/9781836200079.
 
