Chapter 6
233
On the other hand, tasks like the ones previously described require fewer preference pairs. 
Task-specific alignment focuses on improving model performance for a particular function, such 
as modifying the writing style, refusing certain instructions, and so on. These alignments can 
often be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending 
on the task’s complexity.
An example of an application that requires few samples is instructing the model to state that it 
wasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a prefer-
ence dataset, where the rejected answers are those claiming alternative origins, and the chosen 
answers are responses where the model correctly states that it was trained by you. A relatively 
small dataset of 200 to 500 pairs can be enough for this task.
Data generation and evaluation
When creating preference datasets, data generation and evaluation are closely linked. We first 
create answers and then rate them to make the final dataset. In the following, we introduce both 
steps as one process instead of two separate ones.
Generating preferences
Before making new preference data, it’s good to look at relevant open-source datasets. There are 
fewer of these compared to instruction datasets, but you can find high-quality preference data-
sets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. 
Well-known preference datasets include the Anthropic HH-RLHF dataset, which has human 
preferences for helpful and harmless AI responses, and the OpenAI Summarize from Human 
Feedback dataset, which focuses on article summaries.
DPO datasets can be created using various methods, each with its own trade-offs between quality, 
cost, and scalability. These methods can be tailored to specific applications and require varying 
degrees of human feedback. We divide them into four main categories:
•	
Human-generated, human-evaluated datasets: This method involves hiring people to 
both create responses to prompts and evaluate the quality of these responses. While this 
approach can capture nuanced human preferences and is ideal for complex tasks, it’s 
extremely resource-intensive and difficult to scale. As a result, it’s primarily used by large 
AI companies with substantial resources.
•	
Human-generated, LLM-evaluated datasets: This method can be useful if you have 
a lot of existing human-generated content. However, it’s rarely used in practice due to 
inefficiency, as it still requires significant human input for response generation while 
potentially missing nuanced preferences during the LLM evaluation stage.
