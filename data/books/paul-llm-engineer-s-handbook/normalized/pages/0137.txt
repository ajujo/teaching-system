RAG Feature Pipeline
106
The final prompt results from a system and prompt template populated with the user's query and
retrieved context. You might have a single prompt template or multiple prompt templates, depending on your application. Usually, all the prompt engineering is done at the prompt template level.
Below, you can see a dummy example of what a generic system and prompt template look like
and how they are used together with the retrieval logic and the LLM to generate the final answer:
system_template = """
You are a helpful assistant who answers all the user's questions politely.
"""
prompt_template = """
Answer the user's question using only the provided context. If you cannot
answer using the context, respond with "I don't know."
Context: {context}
User question: {user_question}
"""
user_question = "<your_question>"
retrieved_context = retrieve(user_question)
prompt = f"{system_template}\n"
prompt += prompt_template.format(context=retrieved_context, user_
question=user_question)
answer = llm(prompt)
As the prompt templates evolve, each change should be tracked and versioned using machine
learning operations (MLOps) best practices. Thus, during training or inference time, you always
know that a given answer was generated by a specific version of the LLM and prompt template(s).
You can do this through Git, store the prompt templates in a DB, or use specific prompt management tools such as LangFuse.
As we've seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of
your RAG system are the embeddings of the external data, usually stored in vector DBs, the embedding of the user's query, and how we can find similarities between the two using functions
such as the cosine distance. To better understand this part of the RAG algorithm, let's zoom in
on what embeddings are and how they are computed.
