RAG Inference Pipeline
334
                        )
                    ]
                ),
        )
In essence, while plain vector search provides a foundation for semantic retrieval, its limitations
can slow performance in practical applications. Filtered vector search addresses these challenges
by combining the strengths of vector embeddings with contextual filtering, resulting in more
accurate and efficient information retrieval in RAG systems. The last step for optimizing our RAG
pipeline is to look into reranking.
Advanced RAG post-retrieval optimization: reranking
The problem in RAG systems is that the retrieved context may contain irrelevant chunks that only:
•
Add noise: The retrieved context might be irrelevant, cluttering the information and
potentially confusing the language model.
•
Make the prompt bigger: Including unnecessary chunks increases the prompt size, leading to higher costs. Moreover, language models are usually biased toward the context's
first and last pieces. So, if you add a large amount of context, there's a big chance it will
miss the essence.
•
Be come unaligned with your question: Chunks are retrieved based on the similarity
between the query and chunk embeddings. The issue is that the embedding model might
not be tuned to your question, resulting in high similarity scores for chunks that aren't
entirely relevant.
The solution is to use reranking to order all the N × K retrieved chunks based on their relevance
relative to the initial question, where the first chunk will be the most relevant and the last the
least. N represents the number of searches after query expansion, while K is the number of chunks
retrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves
as a critical post-retrieval step that refines the initial results obtained from the retrieval model.
We assess each chunk's relevance to the original query by applying the reranking algorithm, which
often uses advanced models like neural cross-encoders. These models evaluate the semantic similarity between the query and each chunk more accurately than initial retrieval methods based
on embeddings and the cosine similarity distance, as explained in more detail in Chapter 4 in the
An overview of advanced RAG section.
