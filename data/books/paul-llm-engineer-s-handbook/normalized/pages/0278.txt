Chapter 6
247
The learned reward model serves as a proxy for human preferences, enabling the RL algorithm
to train continuously without direct human input for every action.
As an example, Figure 6.4 shows a high-level view of the Proximal Policy Optimization (PPO)
algorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to
score the text that is generated by the trained model. This reward is regularized by an additional
Kullback-Leibler (KL) divergence factor, ensuring that the distribution of tokens stays similar
to the model before training (frozen model).
Figure 6.4 - High-level view of the PPO algorithm for preference alignment
While RLHF has proven effective for aligning AI systems with human preferences, it faces challenges due to its iterative nature and reliance on a separate reward model, which can be computationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms
have also experimentally underperformed compared to simpler approaches. One such approach
that has gained significant attention is DPO.
