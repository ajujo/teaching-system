Chapter 1
21
Training pipeline
The training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with
it, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct
dataset is available in the logical feature store, we will trigger the training pipeline, consume the
artifact, and fine-tune the LLM.
In the initial stages, the data science team owns this step. They run multiple experiments to find
the best model and hyperparameters for the job, either through automatic hyperparameter tuning
or manually. To compare and pick the best set of hyperparameters, we will use an experiment
tracker to log everything of value and compare it between experiments. Ultimately, they will pick
the best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate.
The proposed LLM is then stored in the model registry. After the experimentation phase is over,
we store and reuse the best hyperparameters found to eliminate the manual restrictions of the
process. Now, we can completely automate the training process, known as continuous training.
The testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before
pushing the new model to production, assessing it against a stricter set of tests is critical to see
that the latest candidate is better than what is currently in production. If this step passes, the
model is ultimately tagged as accepted and deployed to the production inference pipeline. Even
in a fully automated ML system, it is recommended to have a manual step before accepting a new
production model. It is like pushing the red button before a significant action with high consequences. Thus, at this stage, an expert looks at a report generated by the testing component. If
everything looks good, it approves the model, and the automation can continue.
The particularities of this component will be on LLM aspects, such as the following:
•
How do you implement an LLM agnostic pipeline?
•
What fine-tuning techniques should you use?
•
How do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?
•
How do you pick an LLM production candidate from multiple experiments?
•
How do you test the LLM to decide whether to push it to production or not?
By the end of this book, you will know how to answer all these questions.
One last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML
orchestrator to schedule and trigger different system parts. For example, we can schedule the
data collection pipeline to crawl data every week.
