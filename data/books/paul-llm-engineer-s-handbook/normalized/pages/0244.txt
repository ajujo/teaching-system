Chapter 5
213
LoRA
LoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the computational challenges associated with adapting massive neural networks, LoRA has quickly become
a cornerstone technique in LLM fine-tuning.
The primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced
computational resources. This is achieved by introducing trainable low-rank matrices that modify the behavior of the model without changing its original parameters. The key advantages of
LoRA include:
â€¢
Dramatically reduced memory usage during training
â€¢
Faster fine-tuning process
â€¢
Preservation of pre-trained model weights (non-destructive)
â€¢
Ability to switch between tasks efficiently by swapping LoRA weights
These benefits have made LoRA particularly attractive for researchers and developers working
with limited computational resources, effectively democratizing the process of LLM fine-tuning.
At its core, LoRA employs a low-rank decomposition technique to update model weights efficiently.
Instead of directly modifying the original weight matrix ğ‘Šğ‘Š, LoRA introduces two smaller matrices,
ğ´ğ´ and ğµğµ, which together form a low-rank update to ğ‘Šğ‘Š.
Figure 5.10 - LoRA adds the two trainable matrices ğ´ğ´ and ğµğµ and keeps the pre-trained weights
ğ‘Šğ‘Š frozen
