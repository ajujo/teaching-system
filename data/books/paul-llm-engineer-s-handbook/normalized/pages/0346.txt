Chapter 8
315
Way back in Chapter 4, we focused only on implementing the ingestion pipeline, which is just
one component of a standard RAG application. In the next chapter, we will conclude the RAG
system by implementing the retrieval and generation components and integrating them into
the inference pipeline.
References
•
Hugging Face, Text Generation Inference, https://github.com/huggingface/textgeneration-inference, 2022.
•
W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.
•
Nvidia, TensorRT-LLM, https://github.com/NVIDIA/TensorRT-LLM, 2023.
•
Y. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding,
2023.
•
T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024.
•
W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.
•
R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith,
O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at
Unprecedented Scale, 2022.
•
Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu,
Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.
•
K. James Reed, PiPPy: Pipeline Parallelism for PyTorch, https://github.com/pytorch/PiPPy,
2022.
•
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training
Multi-Billion Parameter Language Models Using Model Parallelism, 2020.
•
Verma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer
Technical Blog, https://developer.nvidia.com/blog/mastering-llm-techniquesinference-optimization/, 2023.
