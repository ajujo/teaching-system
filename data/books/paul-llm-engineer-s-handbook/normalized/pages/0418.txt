Chapter 10
387
    except Exception:
        logger.exception("SageMaker inference failed.")
        raise
In this method, the inference method constructs the request to be sent to the SageMaker endpoint.
The method packages the payload and other necessary details into a format SageMaker expects. If
an inference_component_name is specified, it is included in the request, allowing for more granular
control over the inference process if needed. The request is sent using the invoke_endpoint()
function, and the response is read, decoded, and returned as a JSON object.
Let's understand how the InferenceExecutor uses the LLMInferenceSagemakerEndpoint class
we previously presented to send HTTP requests to the AWS SageMaker endpoint.
The InferenceExecutor class begins with the constructor, which inputs key parameters for calling
the LLM. The llm parameter accepts any instance that implements the Inference interface, such
as the LLMInferenceSagemakerEndpoint class, which is used to perform the inference.
Also, it accepts the query parameter, which represents the user input. Ultimately, it takes an
optional context field if you want to do RAG, and you can customize the prompt template. If no
prompt template is provided, it will default to a generic version that is not specialized in any LLM:
class InferenceExecutor:
    def __init__(
        self,
        llm: Inference,
        query: str,
        context: str | None = None,
        prompt: str | None = None,
    ) -> None:
        self.llm = llm
        self.query = query
        self.context = context if context else ""
        if prompt is None:
            self.prompt = """
    You are a content creator. Write what the user asked you to while
using the provided context as the primary source of information for the
content.
