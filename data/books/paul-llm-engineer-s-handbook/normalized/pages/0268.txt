Chapter 6
237
However, it's important to note that LLM-based evaluation can be subject to several types of bias:
•
Position bias: In relative scoring, LLM judges tend to favor the first answer presented.
This bias can skew results and lead to inaccurate preferences.
•
Length bias: Similar to humans, LLM judges often show a preference for longer answers,
potentially overlooking the quality of shorter, more concise responses.
•
Family bias: LLM judges may favor responses that are generated by themselves or models
from the same family, potentially due to similarities in language patterns or knowledge
bases.
To mitigate these biases and enhance the quality of preference datasets, several solutions can
be implemented. One key approach is to randomize the order of answer A and answer B in each
comparison, which can counteract position bias by ensuring that the order of presentation doesn't
consistently influence the evaluation. Another valuable strategy involves providing few-shot
examples that demonstrate a balanced distribution of scores. These examples serve to calibrate
the judge LLM's internal scoring mechanism and can effectively address both length and family
bias by illustrating that shorter answers or those from different model families can also be of
high quality. Additionally, employing multiple models as a jury, rather than relying on a single
LLM judge, can significantly improve the robustness of the evaluation process. This multi-model
approach helps to balance out individual biases that may be present in any single model, leading
to a more comprehensive and accurate assessment of the responses.
In the next section, we will create our own preference dataset. We will rely on the data generation
process to naturally create chosen (human-generated) and rejected (LLM-generated) answers.
Creating our own preference dataset
Our model can currently write paragraphs about topics related to machine learning, but it doesn't
have the same writing style as the original authors. This is a typical use case for preference alignment, where we want to change the "voice" of the model to closely imitate the source data. It's
important to note that, experimentally, DPO tends to make models more verbose and pushes
them to use very formal language. Therefore, the training will need to use DPO surgically to avoid
this pitfall and instead adopt the less formal style of these blog articles.
In this section, we will create a preference dataset where the chosen answers are extracts from
the text, while rejected answers are generated by the model. To implement it, we will modify the
code created in Chapter 5, which was designed to generate instruction datasets.
