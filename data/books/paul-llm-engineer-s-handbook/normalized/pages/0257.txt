Supervised Fine-Tuning
226
In particular, three of these metrics are important to monitor:
•
Training loss: It measures how well the model is performing on the task it's being trained
for. The loss should continuously decrease on average, indicating improving performance.
We expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes
and continuous increases in the loss value are signs that the training is failing. In this
case, you might want to check the quality of your data, issues with the tokenizer, and
tune parameters like learning rate and batch size. In Figure 5.11 (loss), you can see three
different phases corresponding to our three epochs.
•
Validation loss: It measures the loss using the validation set instead of the training set;
a well-fitted model typically shows both training and validation losses decreasing and
eventually stabilizing, with a small gap between them. This gap should be minimal but
is expected to exist as the model will always perform slightly better on the training data.
If the training loss continues to decrease while the validation loss starts to increase, it's a
sign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it
indicates underfitting. There are no universal "recommended ranges" for loss values, as
these depend on the specific problem and loss function used. However, you should look for
convergence and stability in both curves. In Figure 4.11 (eval_loss), we see a slight increase
at step 340. This is still acceptable but might indicate that the model starts to overfit.
•
Gradient norm: It represents the magnitude of the gradient vector during training. Large
gradient norms can indicate training instability like overfitting, especially if accompanied
by a divergence between training and validation losses. On the other hand, a stable or
decreasing gradient norm generally means that the model is converging toward a local
optimum. To mitigate issues associated with large gradient norms, gradient clipping can
be employed. This technique involves setting a maximum threshold for the gradient norm,
effectively limiting the size of parameter updates.
It is often interesting to try different learning rates and select the best model based on the minimal
loss. Note that this is a proxy for real evaluations, which are covered in the next chapter.
Summary
This chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We examined the instruction data pipeline and how to create high-quality datasets, from curation
to augmentation. Each pipeline stage offers optimization opportunities, particularly in quality
assessment, data generation, and enhancement. This flexible pipeline can be adapted to your use
cases by selecting the most relevant stages and techniques.
