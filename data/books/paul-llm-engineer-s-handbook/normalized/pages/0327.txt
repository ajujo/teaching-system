Inference Optimization
296
•
Feed these speculative completions into the full model to validate which predictions
match what the large model would have generated.
•
Retain the longest matching prefix from the speculative completions and discard any
incorrect tokens.
The result is that, if the small model approximates the large model well, multiple tokens can be
generated in a single step. This avoids running the expensive large model for several iterations.
The degree of speedup depends on the quality of the small model's predictions - a 90% match
could result in a 3-4X speedup.
It is crucial that both models use the same tokenizer. If this is not the case, the tokens generated by
the draft model will not align with those produced by the large model, making them incompatible.
Let's implement this using the transformers library. In this example, we will use two Qwen1.5
models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the draft
model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, 72B,
or 110B as the main model. Here, we're limited by the VRAM of the T4 GPU in Google Colab, but
to get the maximum speedup, the assistant model should be much smaller than the large model.
Here's a step-by-step guide to implement speculative decoding:
1.
We load the tokenizer and both models:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_
map="auto")
draft_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-
0.5B-Chat", device_map="auto")
2.	 We then tokenize the same input and store it in the accelerator, if available:
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = tokenizer("What is 2+2?", return_tensors="pt").to(device)
