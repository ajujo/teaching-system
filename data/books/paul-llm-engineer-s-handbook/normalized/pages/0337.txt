Inference Optimization
306
However, we are not restricted to these three data types. Lower-precision data types, such as INT8
(8-bit integers), can be employed for quantization, further reducing the memory footprint. NaÃ¯ve
quantization techniques, such as absolute maximum (absmax) quantization and zero-point quantization, can be applied to convert FP32, FP16, or BF16 weights to INT8, as illustrated in Figure 8.10:
Figure 8.10 - Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point
quantization
Absmax quantization maps the original weights ğ—ğ— to the range [-127, 127] by dividing them by the
absolute maximum value of ğ—ğ— and scaling them:
ğ—ğ—quant = round (127 â‹…ğ—ğ—
max|ğ—ğ—|)
For example, if our absolute maximum value is 3.2 (see Figure 8.8), a weight of 0.1 would be
quantized to round (
127â‹…0.1
3.2 ) = 4. To dequantize it, we do the inverse operation:
ğ—ğ—dequant = max|ğ—ğ—| â‹…ğ—ğ—quant
127
This means that if we dequantize our weight, we obtain
3.2â‹…4
127 â‰ˆ0.1008. We can see a rounding error of 0.0008 in this example. In Python, we can implement it as follows with the PyTorch library:
import torch
def absmax_quantize(X):
    # Calculate scale
    scale = 127 / torch.max(torch.abs(X))
    # Quantize
    X_quant = (scale * X).round()
    return X_quant.to(torch.int8)
